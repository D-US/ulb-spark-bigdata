{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 12\n",
    "\n",
    "The youtube video presentation: https://youtu.be/9Rx7i3e21ks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 \n",
    "Implement a model to return the prediction required by the competition. Students are free to reuse\n",
    "existing code available in open access Spark libraries. This step should also provide some quanti-\n",
    "tative assessment of the quality of the set of features returned in the previous step. The learning\n",
    "procedure must be detailed in the notebook. The text should justify the choice of this procedure,\n",
    "assess its accuracy with respect to the one developed in the point 2 and discuss the results. The use\n",
    "of figures, formulas, tables and pseudo-code to describe the combination of this novel procedure is\n",
    "strongly encouraged. Note one third of the score will be attributed on the basis of the quality of the\n",
    "documentation. On the basis of the described procedure, the team must compute the predictions for\n",
    "the competition and submit them via the RecSys website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "<p>\n",
    "It should be noted that due we have sometimes work with samples of data that are less than 1000 to make sure we can easily test our code and analyse the result\n",
    "</p>\n",
    "\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# Disable warnings, set Matplotlib inline plotting and load Pandas package\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is needed to start a Spark session from the notebook\n",
    "import os \n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=4g  pyspark-shell\"\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/03 21:28:53 WARN Utils: Your hostname, steve-Lenovo-ideapad-310-15ABR resolves to a loopback address: 127.0.1.1; using 192.168.2.11 instead (on interface wlp1s0)\n",
      "22/06/03 21:28:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/steve/anaconda3/envs/lab_env/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/06/03 21:28:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#os.environ['PYSPARK_SUBMIT_ARGS'] = \"--conf spark.driver.memory=2g  pyspark-shell\"\n",
    "#http://localhost:4040/jobs/\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start Spark session with local master and 2 cores\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[3]\") \\\n",
    "    .appName(\"q3_spark_3\") \\\n",
    "    .getOrCreate()\n",
    "###############\n",
    "#ON THE CLUSTER OF ULB\n",
    "#spark = SparkSession \\\n",
    " #   .builder \\\n",
    "  #  .master(\"local[2]\") \\\n",
    "   # .appName(\"AppName_ul\") \\\n",
    "    #.config('spark.ui.port', '4050')\\\n",
    "    #.getOrCreate()\n",
    "#####################\n",
    "# Let us retrieve the sparkContext object\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyspark_dist_explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to add new column with default value\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "#import the count function\n",
    "from pyspark.sql.functions import count\n",
    "# Import pyspark.sql.functions as F\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql as SQL\n",
    "#https://github.com/Bergvca/pyspark_dist_explore\n",
    "from pyspark_dist_explore import hist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import DoubleType, ArrayType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "from pyspark.sql.window import Window as W\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    " \n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Work on the data provided from Q1 and Q2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#feature_engineered\n",
    "x_df = spark.read.load('./X_update.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n",
    "#The items that were viewed in a session.\n",
    "y_df = spark.read.load('./Y.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n",
    "#final_test.csv\n",
    "final_test_df = spark.read.load('./final_test.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n",
    "#The candidate items to recommend from. This is one list for both the validation and test set.\n",
    "candidate_items = spark.read.load('./candidate_items.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n",
    "#leaderboard_test.csv\n",
    "leaderboard_test_df = spark.read.load('./leaderboard_test.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n",
    "#result_cross_validation.csv\n",
    "result_cross_validation_df = spark.read.load('./result_cross_validation.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n",
    "\n",
    "feature_score_mrmr_df = spark.read.load('./feature_score_mrmr.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 ['session_id', 'item_id', 'number_of_sessions', 'number_of_item_per_session', 'number_of_items', 'number_of_session_per_item', 'number_of_categories', 'number_of_values', 'number_of_categories_for_session', 'number_of_values_for_session', 'category_id: 1', 'category_id: 2', 'category_id: 3', 'category_id: 4', 'category_id: 5', 'category_id: 6', 'category_id: 7', 'category_id: 8', 'category_id: 9', 'category_id: 10', 'category_id: 11', 'category_id: 12', 'category_id: 13', 'category_id: 14', 'category_id: 15', 'category_id: 16', 'category_id: 17', 'category_id: 18', 'category_id: 19', 'category_id: 20', 'category_id: 21', 'category_id: 22', 'category_id: 23', 'category_id: 24', 'category_id: 25', 'category_id: 26', 'category_id: 27', 'category_id: 28', 'category_id: 29', 'category_id: 30', 'category_id: 31', 'category_id: 32', 'category_id: 33', 'category_id: 34', 'category_id: 35', 'category_id: 36', 'category_id: 37', 'category_id: 38', 'category_id: 39', 'category_id: 40', 'category_id: 41', 'category_id: 42', 'category_id: 43', 'category_id: 44', 'category_id: 45', 'category_id: 46', 'category_id: 47', 'category_id: 48', 'category_id: 49', 'category_id: 50', 'category_id: 51', 'category_id: 52', 'category_id: 53', 'category_id: 54', 'category_id: 55', 'category_id: 56', 'category_id: 57', 'category_id: 58', 'category_id: 59', 'category_id: 60', 'category_id: 61', 'category_id: 62', 'category_id: 63', 'category_id: 64', 'category_id: 65', 'category_id: 66', 'category_id: 67', 'category_id: 68', 'category_id: 69', 'category_id: 70', 'category_id: 71', 'category_id: 72', 'category_id: 73']\n"
     ]
    }
   ],
   "source": [
    "print(len(x_df.columns), x_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 ['session_id', 'item_id', 'number_of_sessions', 'number_of_item_per_session', 'number_of_items', 'number_of_session_per_item', 'number_of_categories', 'number_of_values', 'number_of_categories_for_session', 'number_of_values_for_session', 'category_id: 1', 'category_id: 2', 'category_id: 3', 'category_id: 4', 'category_id: 5', 'category_id: 6', 'category_id: 7', 'category_id: 8', 'category_id: 9', 'category_id: 10', 'category_id: 11', 'category_id: 12', 'category_id: 13', 'category_id: 14', 'category_id: 15', 'category_id: 16', 'category_id: 17', 'category_id: 18', 'category_id: 19', 'category_id: 20', 'category_id: 21', 'category_id: 22', 'category_id: 23', 'category_id: 24', 'category_id: 25', 'category_id: 26', 'category_id: 27', 'category_id: 28', 'category_id: 29', 'category_id: 30', 'category_id: 31', 'category_id: 32', 'category_id: 33', 'category_id: 34', 'category_id: 35', 'category_id: 36', 'category_id: 37', 'category_id: 38', 'category_id: 39', 'category_id: 40', 'category_id: 41', 'category_id: 42', 'category_id: 43', 'category_id: 44', 'category_id: 45', 'category_id: 46', 'category_id: 47', 'category_id: 48', 'category_id: 49', 'category_id: 50', 'category_id: 51', 'category_id: 52', 'category_id: 53', 'category_id: 54', 'category_id: 55', 'category_id: 56', 'category_id: 57', 'category_id: 58', 'category_id: 59', 'category_id: 60', 'category_id: 61', 'category_id: 62', 'category_id: 63', 'category_id: 64', 'category_id: 65', 'category_id: 66', 'category_id: 67', 'category_id: 68', 'category_id: 69', 'category_id: 70', 'category_id: 71', 'category_id: 72', 'category_id: 73']\n"
     ]
    }
   ],
   "source": [
    "print(len(x_df.columns), final_test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5743820, 83)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print((x_df.count(), len(x_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------+---------------+\n",
      "|_c0|              scores|col_id|       col_name|\n",
      "+---+--------------------+------+---------------+\n",
      "|  0|0.046080561980858796|    32|category_id: 24|\n",
      "+---+--------------------+------+---------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/03 21:30:41 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , scores, col_id, col_name\n",
      " Schema: _c0, scores, col_id, col_name\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/steve/Documents/COURSES/PROJECTS/Big%20data%20Distributed%20system-project/doc/feature_score_mrmr.csv\n"
     ]
    }
   ],
   "source": [
    "feature_score_mrmr_df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the list of the most ranked columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['category_id: 24', 'number_of_sessions', 'category_id: 16', 'category_id: 70', 'category_id: 51', 'category_id: 20', 'category_id: 54', 'category_id: 22', 'category_id: 48', 'category_id: 12', 'category_id: 6', 'category_id: 11', 'category_id: 40', 'category_id: 10', 'category_id: 72', 'category_id: 14', 'number_of_categories_for_session', 'category_id: 44', 'category_id: 56', 'category_id: 1', 'category_id: 21', 'category_id: 37', 'category_id: 60', 'category_id: 18', 'category_id: 61', 'category_id: 29', 'number_of_item_per_session', 'category_id: 66', 'category_id: 57', 'category_id: 53', 'category_id: 30', 'category_id: 55', 'category_id: 50', 'category_id: 28', 'category_id: 59', 'number_of_values_for_session', 'category_id: 38', 'category_id: 68', 'item_id', 'category_id: 41', 'number_of_items', 'category_id: 8', 'category_id: 33', 'category_id: 42', 'category_id: 34', 'number_of_session_per_item', 'category_id: 25', 'category_id: 62', 'category_id: 7', 'category_id: 19', 'category_id: 4', 'category_id: 69', 'category_id: 15', 'category_id: 58', 'category_id: 73', 'number_of_values', 'category_id: 47', 'number_of_categories', 'category_id: 65', 'category_id: 67', 'category_id: 26', 'category_id: 49', 'category_id: 32', 'category_id: 63', 'category_id: 23', 'category_id: 17', 'category_id: 2', 'category_id: 45', 'category_id: 39', 'category_id: 3', 'category_id: 31', 'category_id: 5', 'category_id: 36', 'category_id: 46']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "feature_score_mrmr_list = feature_score_mrmr_df.select(\"col_name\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "print (feature_score_mrmr_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joined the X and Y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#We select the first 10000 to work with the data\n",
    "limit_val = 1000\n",
    "df_features = x_df.limit(limit_val)\n",
    "df_label = y_df.select('purchased').limit(limit_val)\n",
    "index_list = list(range(1, 1+df_label.count() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###### Add an index column to the spark dataframe\n",
    "\n",
    "#### For the label_df\n",
    "#convert list to a dataframe\n",
    "\n",
    "def build_joined_data_from_x_y(dataframeX, dataframeY):\n",
    "    index_df = sqlContext.createDataFrame([(l,) for l in index_list], ['index'])\n",
    "    dataframeY = dataframeY.withColumn(\"idx\", F.monotonically_increasing_id())\n",
    "    dataframeX = dataframeX.withColumn(\"idx\", F.monotonically_increasing_id())\n",
    "    index_df = index_df.withColumn(\"idx\", F.monotonically_increasing_id())\n",
    "\n",
    "    windowSpec = W.orderBy(\"idx\")\n",
    "    dataframeY = dataframeY.withColumn(\"idx\", F.row_number().over(windowSpec))\n",
    "    dataframeX = dataframeX.withColumn(\"idx\", F.row_number().over(windowSpec))\n",
    "    index_df = index_df.withColumn(\"idx\", F.row_number().over(windowSpec))\n",
    "    \n",
    "    #dataframeY = dataframeY.withColumnRenamed('idx', 'idx2')\n",
    "    joined_data = dataframeX.join(dataframeY,\n",
    "              dataframeX[\"idx\"] ==  dataframeY[\"idx\"] \n",
    "                                  ,\"inner\")\n",
    "    joined_data = joined_data.drop(\"idx\")\n",
    "    return joined_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['session_id', 'item_id', 'number_of_sessions', 'number_of_item_per_session', 'number_of_items', 'number_of_session_per_item', 'number_of_categories', 'number_of_values', 'number_of_categories_for_session', 'number_of_values_for_session', 'category_id: 1', 'category_id: 2', 'category_id: 3', 'category_id: 4', 'category_id: 5', 'category_id: 6', 'category_id: 7', 'category_id: 8', 'category_id: 9', 'category_id: 10', 'category_id: 11', 'category_id: 12', 'category_id: 13', 'category_id: 14', 'category_id: 15', 'category_id: 16', 'category_id: 17', 'category_id: 18', 'category_id: 19', 'category_id: 20', 'category_id: 21', 'category_id: 22', 'category_id: 23', 'category_id: 24', 'category_id: 25', 'category_id: 26', 'category_id: 27', 'category_id: 28', 'category_id: 29', 'category_id: 30', 'category_id: 31', 'category_id: 32', 'category_id: 33', 'category_id: 34', 'category_id: 35', 'category_id: 36', 'category_id: 37', 'category_id: 38', 'category_id: 39', 'category_id: 40', 'category_id: 41', 'category_id: 42', 'category_id: 43', 'category_id: 44', 'category_id: 45', 'category_id: 46', 'category_id: 47', 'category_id: 48', 'category_id: 49', 'category_id: 50', 'category_id: 51', 'category_id: 52', 'category_id: 53', 'category_id: 54', 'category_id: 55', 'category_id: 56', 'category_id: 57', 'category_id: 58', 'category_id: 59', 'category_id: 60', 'category_id: 61', 'category_id: 62', 'category_id: 63', 'category_id: 64', 'category_id: 65', 'category_id: 66', 'category_id: 67', 'category_id: 68', 'category_id: 69', 'category_id: 70', 'category_id: 71', 'category_id: 72', 'category_id: 73', 'purchased']\n"
     ]
    }
   ],
   "source": [
    "joined_data = build_joined_data_from_x_y(df_features, df_label)\n",
    "print(joined_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_joined_df_session_id(sessionid, joined_df):\n",
    "    temp_df = joined_df.filter(joined_df.session_id == sessionid)\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_x_and_y_df_session_id(sessionid, joined_df):\n",
    "    temp_df = return_joined_df_session_id(sessionid, joined_df)\n",
    "    cols = joined_df.columns\n",
    "    cols.remove('session_id')\n",
    "    cols.remove('purchased')\n",
    "    xtemp_df = temp_df.select(cols)\n",
    "    ytemp_df = temp_df.select('purchased')\n",
    "    return xtemp_df, ytemp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/03 21:30:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/03 21:30:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 22:=======================================>                 (7 + 3) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|session_id|purchased|\n",
      "+----------+---------+\n",
      "|     11897|        0|\n",
      "|    517715|        0|\n",
      "|    630526|        0|\n",
      "|    693471|        1|\n",
      "|    922323|        1|\n",
      "|   1059784|        0|\n",
      "|   1139237|        0|\n",
      "|   1535716|        0|\n",
      "|   1558237|        0|\n",
      "|   1593430|        1|\n",
      "+----------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "joined_data.limit(10).select('session_id', 'purchased').distinct().show(20)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Random forest on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/46710934/pyspark-sql-utils-illegalargumentexception-ufield-features-does-not-exist\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "def construct_ml_features_col(feat_cols_list, featuresDF):\n",
    "    if 'index' in feat_cols_list:\n",
    "        feat_cols_list.remove('index')\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=feat_cols_list,\n",
    "        outputCol='features')\n",
    "    dataset = assembler.transform(featuresDF)\n",
    "    #print ('inside construct_ml_features_col **')\n",
    "    #print (dataset.select('features').show(1))\n",
    "    return dataset\n",
    "\n",
    "def construct_join_ml_df(feat_cols_list, featuresDF, labelDF):\n",
    "    dataset = construct_ml_features_col(feat_cols_list, featuresDF)\n",
    "    newdata = labelDF.withColumnRenamed('purchased', 'label')\n",
    "    dataset =  build_joined_data_from_x_y(dataset, newdata)\n",
    "    dataset = dataset.select(col('label'), col('features'))\n",
    "    #print ('inside F2 *** construct_join_ml_df **')\n",
    "    #print(dataset.show(1))\n",
    "    #newdata.join(dataset, newdata.index == dataset.index).drop(\"index\").select(col('label'), col('features'))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the function to train the model for a given session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " \n",
    "def train_df_model_per_session(session_id, joinedData, feat_score_mrmr_df, number_of_features = 10):\n",
    "    data = []\n",
    "     ## Set the number of columns to be used from the ranking algorithm\n",
    "    df_features_cols_2 = feat_score_mrmr_df.select(\"col_name\").limit(number_of_features).rdd.flatMap(lambda x: x).collect()\n",
    "    if not ('item_id' in df_features_cols_2):\n",
    "        df_features_cols_2.append('item_id')\n",
    "        \n",
    "     #get the joined DF for the session Id\n",
    "    joinedDF = return_joined_df_session_id(session_id, joinedData)\n",
    "    \n",
    "    #sample the validation data set\n",
    "    df_sample_validation = joinedDF.sample(withReplacement=False, fraction=0.3, seed=42)\n",
    "    \n",
    "    #sample the training data set \n",
    "    df_sample_training = joinedDF.sample(withReplacement=False, fraction=0.9, seed=42)\n",
    "    \n",
    "    #extract x and y\n",
    "    df_sample_trainingx,  df_sample_trainingy= return_x_and_y_df_session_id(session_id, df_sample_training)\n",
    "    df_sample_validationx,  df_sample_validationy= return_x_and_y_df_session_id(session_id, df_sample_validation)\n",
    "    \n",
    "   \n",
    "    #df_features_cols_2.drop\n",
    "  \n",
    "    #df_features_cols_2 = list(df_sample_trainingx.columns)\n",
    "    \n",
    "    #df_sample_trainingx = df_sample_trainingx.withColumnRenamed('purchased', 'label')\n",
    "    #print(' before dat_training****' )\n",
    "    #print (df_sample_trainingx.show(1))\n",
    "    #dat_training = construct_join_ml_df(df_features_cols_2,df_sample_trainingx, df_sample_trainingy)\n",
    "    #########################\\\n",
    "    \n",
    "    df_sample_training = construct_ml_features_col(df_features_cols_2, df_sample_training)\n",
    "    dat_training = df_sample_training.withColumnRenamed('purchased', 'label')\n",
    "    dat_training = dat_training.select(col('label'), col('features'))\n",
    "    ###################################\n",
    "    #print('sample dattraining = ', dat_training.count() )\n",
    "    #print(dat_training.show(2))\n",
    "        \n",
    "    rf = RandomForestClassifier( labelCol=\"label\", featuresCol=\"features\", seed=0 )\n",
    "    rf = rf.fit(dat_training)\n",
    "   \n",
    "    #######\n",
    "    #print('data for prediction')\n",
    "    #print(df_sample_validationx.show(1))\n",
    "    \n",
    "    predict_data =  construct_ml_features_col(df_features_cols_2, df_sample_validationx).select(col('features'))\n",
    "    #print('after tranformation data for prediction')\n",
    "    #print(predict_data.show(1))\n",
    "    #####\n",
    "    pred = rf.transform(predict_data)\n",
    "\n",
    "    ###\n",
    "    #print('predict_data ', predict_data.count())\n",
    "    #print(df_sample_validationx.show(1))\n",
    "    #print(predict_data.show(1))\n",
    "    ###\n",
    "    array_value = np.array(df_sample_validationy.select(col('purchased')).collect() )\n",
    "    array_prediction_2 = np.array(pred.select(col('prediction')).collect())\n",
    "    val_2 = np.mean( np.power( array_prediction_2 - array_value  , 2 ) )\n",
    "\n",
    "    data.append( (session_id, val_2.item()  ) )\n",
    "        #print(\"#Features: \", (j+1), X_tr.columns[0:(j+1)], \" ; CV error LR =\", CV_err[j][i] , \" ; CV error DT =\", CV_err_2[j][i] )\n",
    "\n",
    "    columns = ['session_id' ,\"mse\" ]\n",
    "    rdd = spark.sparkContext.parallelize(data)\n",
    "    result_cv = spark.createDataFrame(rdd).toDF(*columns)\n",
    "    #print(pred.printSchema()) \n",
    "    pred2 = pred.select( split_array_to_list(F.col(\"probability\")).alias(\"prob\"))\\\n",
    "                            .select([F.col(\"prob\")[i] for i in range(2)])\n",
    "    \n",
    "    pred = build_joined_data_from_x_y(pred, pred2)\n",
    "    \n",
    "    ##confusion matrix\n",
    "    #select only prediction and label columns\n",
    "    y_true = array_value#pred.select(['label']).collect()\n",
    "    y_pred = pred.select(['prediction']).collect()\n",
    "    \n",
    "    #print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    #metrics_rf =  [classification_report(y_true, y_pred), confusion_matrix(y_true, y_pred)]\n",
    "\n",
    "    return pred, result_cv, rf, df_features_cols_2#, metrics_rf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_array_to_list(col):\n",
    "    def to_list(v):\n",
    "        return v.toArray().tolist()\n",
    "    return F.udf(to_list, ArrayType(DoubleType()))(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/03 21:30:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/03 21:30:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/03 21:31:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/03 21:31:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/03 21:31:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/03 21:31:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/03 21:31:05 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 1 (= number of training instances)\n",
      "22/06/03 21:31:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/03 21:31:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/03 21:31:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/03 21:31:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/03 21:31:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/03 21:31:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/03 21:31:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/03 21:31:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/03 21:31:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/03 21:31:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/03 21:31:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/03 21:31:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/03 21:31:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/03 21:31:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/03 21:31:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/03 21:31:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/03 21:31:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/03 21:31:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/03 21:31:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/03 21:31:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----------+----------+-------+-------+\n",
      "|features|rawPrediction|probability|prediction|prob[0]|prob[1]|\n",
      "+--------+-------------+-----------+----------+-------+-------+\n",
      "+--------+-------------+-----------+----------+-------+-------+\n",
      "\n",
      "None\n",
      "+----------+---+\n",
      "|session_id|mse|\n",
      "+----------+---+\n",
      "|    922323|NaN|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_rf, result_rf, rf_model, features_list = train_df_model_per_session(922323, joined_data.limit(1000), feature_score_mrmr_df, number_of_features=1 )\n",
    "print(pred_rf.show())\n",
    "result_rf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|session_id|item_id|\n",
      "+----------+-------+\n",
      "|    488622|      3|\n",
      "|     81892|      9|\n",
      "|     92071|      9|\n",
      "|    257553|      9|\n",
      "+----------+-------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_test_df.select('session_id', 'item_id').limit(100).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/03 21:31:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/03 21:31:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/03 21:31:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/03 21:31:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------+-------------+-----------+----------+-------+-------+\n",
      "|session_id|item_id| features|rawPrediction|probability|prediction|prob[0]|prob[1]|\n",
      "+----------+-------+---------+-------------+-----------+----------+-------+-------+\n",
      "|    488622|      3|[0.0,3.0]|   [0.0,11.0]|  [0.0,1.0]|       1.0|    0.0|    1.0|\n",
      "+----------+-------+---------+-------------+-----------+----------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_from_final_session_id(sessionID, final_test_df, rf_model, features_list, k_limit=100):\n",
    "    jd = return_joined_df_session_id(sessionID, final_test_df.limit(k_limit))\n",
    "    predict_data =  construct_ml_features_col(features_list, jd).select(col('features'))\n",
    "    pred2 = rf_model.transform(predict_data)\n",
    "    pred_t= pred2.select( split_array_to_list(F.col(\"probability\")).alias(\"prob\"))\\\n",
    "                                .select([F.col(\"prob\")[i] for i in range(2)])\n",
    "\n",
    "    pred2 = build_joined_data_from_x_y(pred2, pred_t)\n",
    "    tempdf = final_test_df.select('session_id', 'item_id').filter(final_test_df.session_id == sessionID)\n",
    "    pred2 = build_joined_data_from_x_y(tempdf, pred2)\n",
    "    return pred2\n",
    "\n",
    "sessionID= 488622\n",
    "predict_from_final_session_id(sessionID, final_test_df, rf_model, features_list).show()\n",
    "#return a df with column session_id|item_id| features|rawPrediction|probability|prediction|prob[0]|prob[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define the global function which takes a joined dataframe (the training dataset) and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define the function to test the trained model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Apply random forest on the final test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the function which predict the first 100 items of a given session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the steps to produce the 100 predictions for a session:\n",
    "* build a matrix having as column the item id from the candidate_items csv files and the most 10 ranked columns from the MRMR ranking alogrithm (from question 02).\n",
    "* Train the model from the set of training data for only a sigle session id\n",
    "* apply the model on the newly formed matrix of the candidate_items and the most relevant MRMR columns\n",
    "* order the resulting dataframe from descending order (highest to lowest) the probability column and take the first 100\n",
    "* add a ranking column \n",
    "* drop the probability column and return the dataframe having the following column session_id,item_id,rank\n",
    "\n",
    "<i>NB:</i>\n",
    "In fact, from the project instruction, the candidate item has the data of all the item_id that have been bought, and that is why we use this data for the prediction\n",
    "\n",
    "use the final test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_item_id_mrmr_df(x_df, feature_score_mrmr_df, item_id, k_columns=5):\n",
    "    \n",
    "    list_column = feature_score_mrmr_df.select(\"col_name\").limit(k_columns).rdd.flatMap(lambda x: x).collect()\n",
    "    if not ('item_id' in list_column):\n",
    "        list_column.append('item_id')\n",
    "    new_item = x_df.select(list_column)\n",
    "    new_candidate_item = new_item.filter(new_item.item_id == item_id)\n",
    "    #new_candidate_item = new_candidate_item.withColumnRenamed('item_id', 'item_id2')\n",
    "    \n",
    "    return new_candidate_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_score_mrmr_list = feature_score_mrmr_df.select(\"col_name\").rdd.flatMap(lambda x: x).collect()\n",
    "def return_k_bought_itemper_session(session_id, joinedData, feature_score_mrmr_df,  final_test_df, number_of_features=10, value_of_k=2):\n",
    "    pred_rf, result_rf, rf = train_df_model_per_session(session_id, joined_data, feature_score_mrmr_df, number_of_features)\n",
    "    print(pred_rf.show())\n",
    "    result_rf.show()\n",
    "    \n",
    "    ## work on the candidate item\n",
    "    candidate_item_list = candidate_items.select(\"item_id\").limit(value_of_k).rdd.flatMap(lambda x: x).collect()\n",
    "    df_features_cols_2 = feature_score_mrmr_df.select(\"col_name\").limit(number_of_features).rdd.flatMap(lambda x: x).collect()\n",
    "    for item_id in candidate_item_list:\n",
    "        item_id_df =  return_item_id_mrmr_df(final_test_df, feature_score_mrmr_df, item_id=4, k_columns=5)\n",
    "        predict_data =  construct_ml_features_col(df_features_cols_2, item_id_df).select(col('features'))\n",
    "        pred = rf.transform(predict_data)\n",
    "        print(pred)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define the general function which work for all the session id present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
