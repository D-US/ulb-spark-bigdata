{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0090dc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# Disable warnings, set Matplotlib inline plotting and load Pandas package\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8c8292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os \n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib notebook  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32eab089",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is needed to start a Spark session from the notebook\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=100g  pyspark-shell\"\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c63ac1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/03 21:44:36 WARN Utils: Your hostname, Fatemehs-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.178.38 instead (on interface en0)\n",
      "22/06/03 21:44:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/fatemeh/opt/anaconda3/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/03 21:44:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/06/03 21:44:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "#Solution\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"6G\") \\\n",
    "    .appName(\"task_1_1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b66bbc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Load the ./data/churn-bigml-80.csv and store in CV_data\n",
    "train_sessions = spark.read.load('./data/train_sessions.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7ea5b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:>                                                          (0 + 2) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_purchases = spark.read.load('./data/train_purchases.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbe7f3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features = spark.read.load('./data/item_features.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "994f642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding new column to trian_purchase \n",
    "from pyspark.sql.functions import *\n",
    "train_purchases = train_purchases.withColumn(\"purchased\", lit(1))\n",
    "#train_purchases.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2194d855",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sessions = train_sessions.withColumn(\"purchased\", lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "972e5ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_sessions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c426390",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concate the train_session with train_purchased\n",
    "new = train_purchases.union(train_sessions)\n",
    "#new.show()\n",
    "#result = df1.union(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "940eb0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = new.sort(\"session_id\")\n",
    "#new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4116ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of items per session\n",
    "\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as f\n",
    "w = Window.partitionBy('session_id')\n",
    "new = new.select('session_id', 'item_id', 'date', 'purchased', f.count('session_id').over(w).alias('number_of_sessions')).sort('session_id', 'item_id')\n",
    "#new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f29d46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of distinct items per session\n",
    "number_of_item_per_session = new.groupBy(\"session_id\").agg(countDistinct(\"item_id\").alias(\"number_of_item_per_session\"))\n",
    "#number_of_item_per_session.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f214eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merg two dataframes together\n",
    "number_of_item_per_session = number_of_item_per_session.withColumnRenamed(\"session_id\",\"session_id_2\")\n",
    "new = new.join(number_of_item_per_session, new.session_id ==  number_of_item_per_session.session_id_2,\"inner\") \n",
    "new = new.drop(\"session_id_2\")\n",
    "#new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b59c1087",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of items in the set for each item_id\n",
    "w = Window.partitionBy('item_id')\n",
    "new = new.select('session_id', 'item_id', 'date', 'purchased','number_of_sessions','number_of_item_per_session', f.count('item_id').over(w).alias('number_of_items')).sort('session_id', 'item_id')\n",
    "#new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "664e0e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of session for each item\n",
    "number_of_session_per_item = new.groupBy(\"item_id\").agg(countDistinct(\"session_id\").alias(\"number_of_session_per_item\"))\n",
    "number_of_session_per_item = number_of_session_per_item.withColumnRenamed(\"item_id\",\"item_id_2\")\n",
    "new = new.join(number_of_session_per_item, new.item_id ==  number_of_session_per_item.item_id_2,\"inner\") \n",
    "new = new.drop(\"item_id_2\")\n",
    "#new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3643057",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = new.sort(\"session_id\")\n",
    "#new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27564139",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = item_features.groupBy(\"item_id\").agg(countDistinct(\"feature_category_id\").alias(\"number_of_categories\"), countDistinct(\"feature_value_id\").alias(\"number_of_values\"))\n",
    "#a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6190831",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add this dataframe to the original one so we have the number of features and number of values for each item\n",
    "a = item_features.groupBy(\"item_id\").agg(countDistinct(\"feature_category_id\").alias(\"number_of_categories\"), countDistinct(\"feature_value_id\").alias(\"number_of_values\"))\n",
    "a = a.withColumnRenamed(\"item_id\",\"item_id_2\")\n",
    "new = new.join(a, new.item_id ==  a.item_id_2,\"inner\") \n",
    "new = new.drop(\"item_id_2\")\n",
    "#new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c046011",
   "metadata": {},
   "outputs": [],
   "source": [
    "#join sessions with item_features\n",
    "new_train = train_purchases.union(train_sessions)\n",
    "new_train = new_train.drop(\"date\")\n",
    "new_train = new_train.drop(\"purchased\")\n",
    "#new_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30625c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we add session to the item_features dataframe\n",
    "new_train = new_train.withColumnRenamed(\"item_id\",\"item_id_2\")\n",
    "new_train = new_train.join(item_features, new_train.item_id_2 ==  item_features.item_id,\"inner\") \n",
    "new_train = new_train.drop(\"item_id_2\")\n",
    "#new_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27eecd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we count the number of categories and values for each session\n",
    "b = new_train.groupBy(\"session_id\").agg(countDistinct(\"feature_category_id\").alias(\"number_of_categories_for_session\"), countDistinct(\"feature_value_id\").alias(\"number_of_values_for_session\"))\n",
    "#b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fb03d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we add these two features to the original dataframe\n",
    "b = b.withColumnRenamed(\"session_id\",\"session_id_2\")\n",
    "new = new.join(b, new.session_id ==  b.session_id_2,\"inner\") \n",
    "new = new.drop(\"session_id_2\")\n",
    "#new.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b26fb7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start playing with the time\n",
    "#add epoch as a new feature\n",
    "new = new.withColumn( \"epoch_seconds\", \n",
    "      unix_timestamp(col(\"date\"),\"yyyy-MM-dd HH:mm:ss.SSS\").alias(\"date\"))\n",
    "#new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c25ecd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting date string to date\n",
    "new = new.withColumn(\"time\", to_timestamp(new.date, 'yyyy-MM-dd HH:mm:ss.SSS').alias('dt'))\n",
    "#train_sessions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d019dc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofweek\n",
    "new = new.withColumn('dayOfWeek', dayofweek(col('time')))\n",
    "new = new.withColumn('month', month(col('time')))\n",
    "new = new.withColumn('year', year(col('time')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12bd9b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = new.withColumn('hour', hour(col('time')))\n",
    "new = new.withColumn('minute', minute(col('time')))\n",
    "new = new.withColumn('dayofmonth', dayofmonth(col('time')))\n",
    "new = new.withColumn('dayofyear', dayofyear(col('time')))\n",
    "new = new.withColumn('weekofyear', weekofyear(col('time')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47409638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5743820"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafa4cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want to add category ids to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ccda004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features = item_features.withColumnRenamed(\"item_id\",\"item_id_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94c714c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = train_purchases.union(train_sessions)\n",
    "test1 = test1.drop('date')\n",
    "test1 = test1.drop('session_id')\n",
    "test1 = test1.drop('purchased')\n",
    "#test1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6fce5607",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = test1.groupby(\"item_id\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44ad3958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23618"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d810225",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = test2.withColumnRenamed(\"item_id\",\"item_id_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a22393e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6356666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we add category ids as features to our dataframe\n",
    "for i in range(1,74):\n",
    "    #print(i)\n",
    "    cond = (test1.item_id == item_features.item_id_2)\n",
    "    test1 = test1.join(item_features, cond, how=\"inner\").withColumn(\"category_id: \" + str(i), when((col(\"feature_category_id\")==i), col(\"feature_value_id\")).otherwise(0))\n",
    "    test1 = test1.drop(\"item_id_2\")\n",
    "    test1 = test1.drop(\"feature_category_id\")\n",
    "    test1 = test1.drop(\"feature_value_id\")\n",
    "    test1 = test1.groupBy(\"item_id\").agg(F.max(\"category_id: \" + str(i)).alias(\"category_id: \" + str(i)))\n",
    "    test2 = test2.join(test1,test2.item_id_2 == test1.item_id, how = \"inner\")\n",
    "    test2 = test2.drop(\"item_id\")\n",
    "    #print('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b784f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23618"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2ce8ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we merge the dataframe with the category ids with the other one that has date and new features\n",
    "new_final = new.join(test2, new.item_id ==  test2.item_id_2,\"inner\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eca8fd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_final = new_final.drop('date','epoch_seconds','time','dayOfweek',\"month\",'year','item_id_2','count','purchased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6790eb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/02 20:38:58 WARN DAGScheduler: Broadcasting large task binary with size 31.5 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "new_final.toPandas().to_csv('X_update.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ee4a92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('item_id', 'int'),\n",
       " ('number_of_sessions', 'bigint'),\n",
       " ('number_of_item_per_session', 'bigint'),\n",
       " ('number_of_items', 'bigint'),\n",
       " ('number_of_session_per_item', 'bigint'),\n",
       " ('number_of_categories', 'bigint'),\n",
       " ('number_of_values', 'bigint'),\n",
       " ('number_of_categories_for_session', 'bigint'),\n",
       " ('number_of_values_for_session', 'bigint'),\n",
       " ('category_id: 1', 'int'),\n",
       " ('category_id: 2', 'int'),\n",
       " ('category_id: 3', 'int'),\n",
       " ('category_id: 4', 'int'),\n",
       " ('category_id: 5', 'int'),\n",
       " ('category_id: 6', 'int'),\n",
       " ('category_id: 7', 'int'),\n",
       " ('category_id: 8', 'int'),\n",
       " ('category_id: 9', 'int'),\n",
       " ('category_id: 10', 'int'),\n",
       " ('category_id: 11', 'int'),\n",
       " ('category_id: 12', 'int'),\n",
       " ('category_id: 13', 'int'),\n",
       " ('category_id: 14', 'int'),\n",
       " ('category_id: 15', 'int'),\n",
       " ('category_id: 16', 'int'),\n",
       " ('category_id: 17', 'int'),\n",
       " ('category_id: 18', 'int'),\n",
       " ('category_id: 19', 'int'),\n",
       " ('category_id: 20', 'int'),\n",
       " ('category_id: 21', 'int'),\n",
       " ('category_id: 22', 'int'),\n",
       " ('category_id: 23', 'int'),\n",
       " ('category_id: 24', 'int'),\n",
       " ('category_id: 25', 'int'),\n",
       " ('category_id: 26', 'int'),\n",
       " ('category_id: 27', 'int'),\n",
       " ('category_id: 28', 'int'),\n",
       " ('category_id: 29', 'int'),\n",
       " ('category_id: 30', 'int'),\n",
       " ('category_id: 31', 'int'),\n",
       " ('category_id: 32', 'int'),\n",
       " ('category_id: 33', 'int'),\n",
       " ('category_id: 34', 'int'),\n",
       " ('category_id: 35', 'int'),\n",
       " ('category_id: 36', 'int'),\n",
       " ('category_id: 37', 'int'),\n",
       " ('category_id: 38', 'int'),\n",
       " ('category_id: 39', 'int'),\n",
       " ('category_id: 40', 'int'),\n",
       " ('category_id: 41', 'int'),\n",
       " ('category_id: 42', 'int'),\n",
       " ('category_id: 43', 'int'),\n",
       " ('category_id: 44', 'int'),\n",
       " ('category_id: 45', 'int'),\n",
       " ('category_id: 46', 'int'),\n",
       " ('category_id: 47', 'int'),\n",
       " ('category_id: 48', 'int'),\n",
       " ('category_id: 49', 'int'),\n",
       " ('category_id: 50', 'int'),\n",
       " ('category_id: 51', 'int'),\n",
       " ('category_id: 52', 'int'),\n",
       " ('category_id: 53', 'int'),\n",
       " ('category_id: 54', 'int'),\n",
       " ('category_id: 55', 'int'),\n",
       " ('category_id: 56', 'int'),\n",
       " ('category_id: 57', 'int'),\n",
       " ('category_id: 58', 'int'),\n",
       " ('category_id: 59', 'int'),\n",
       " ('category_id: 60', 'int'),\n",
       " ('category_id: 61', 'int'),\n",
       " ('category_id: 62', 'int'),\n",
       " ('category_id: 63', 'int'),\n",
       " ('category_id: 64', 'int'),\n",
       " ('category_id: 65', 'int'),\n",
       " ('category_id: 66', 'int'),\n",
       " ('category_id: 67', 'int'),\n",
       " ('category_id: 68', 'int'),\n",
       " ('category_id: 69', 'int'),\n",
       " ('category_id: 70', 'int'),\n",
       " ('category_id: 71', 'int'),\n",
       " ('category_id: 72', 'int'),\n",
       " ('category_id: 73', 'int')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_final.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "383d16e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    }
   ],
   "source": [
    "#new_final.count()\n",
    "print(len(new_final.columns))\n",
    "\n",
    "#print(new_final.shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c26235a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:================================================>         (5 + 1) / 6]\r"
     ]
    }
   ],
   "source": [
    "#convert dataframe to RDD\n",
    "\n",
    "final_rdd = final.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dd0093",
   "metadata": {},
   "source": [
    "# preparing test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9c0ca11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "X = spark.read.load('X.csv', \n",
    "                     format='com.databricks.spark.csv', \n",
    "                     header='true', \n",
    "                     inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c2b36fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final_sessions = spark.read.load('./data/test_final_sessions.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2eea065",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_leaderboard_sessions = spark.read.load('./data/test_leaderboard_sessions.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "98d58294",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features = spark.read.load('./data/item_features.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "db42cd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as f\n",
    "w = Window.partitionBy('session_id')\n",
    "\n",
    "df_1 = test_leaderboard_sessions.select('session_id', 'item_id', 'date', f.count('session_id').over(w).alias('number_of_sessions')).sort('session_id', 'item_id')\n",
    "#df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "78a9a30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of distinct items per session\n",
    "from pyspark.sql.functions import * \n",
    "df_2 = test_leaderboard_sessions.groupBy(\"session_id\").agg(countDistinct(\"item_id\").alias(\"number_of_item_per_session\"))\n",
    "#df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "11444fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merg two dataframes together\n",
    "df_2 = df_2.withColumnRenamed(\"session_id\",\"session_id_2\")\n",
    "df_1 = df_1.join(df_2, df_1.session_id ==  df_2.session_id_2,\"inner\") \n",
    "df_1 = df_1.drop(\"session_id_2\")\n",
    "#df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fca992c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of items in the set for each item_id\n",
    "w = Window.partitionBy('item_id')\n",
    "df_1 = df_1.select('session_id', 'item_id', 'date','number_of_sessions','number_of_item_per_session', f.count('item_id').over(w).alias('number_of_items')).sort('session_id', 'item_id')\n",
    "#df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d28c2b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of session for each item\n",
    "number_of_session_per_item = df_1.groupBy(\"item_id\").agg(countDistinct(\"session_id\").alias(\"number_of_session_per_item\"))\n",
    "number_of_session_per_item = number_of_session_per_item.withColumnRenamed(\"item_id\",\"item_id_2\")\n",
    "df_1 = df_1.join(number_of_session_per_item, df_1.item_id ==  number_of_session_per_item.item_id_2,\"inner\") \n",
    "df_1 = df_1.drop(\"item_id_2\")\n",
    "#df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "51bcf5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add this dataframe to the original one so we have the number of features and number of values for each item\n",
    "df_3 = item_features.groupBy(\"item_id\").agg(countDistinct(\"feature_category_id\").alias(\"number_of_categories\"), countDistinct(\"feature_value_id\").alias(\"number_of_values\"))\n",
    "df_3 = df_3.withColumnRenamed(\"item_id\",\"item_id_2\")\n",
    "df_1 = df_1.join(df_3, df_1.item_id ==  df_3.item_id_2,\"inner\") \n",
    "df_1 = df_1.drop(\"item_id_2\")\n",
    "#new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7e628859",
   "metadata": {},
   "outputs": [],
   "source": [
    "#join sessions with item_features\n",
    "new_test = test_final_sessions.drop(\"date\")\n",
    "#new_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a31fd6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we add session to the item_features dataframe\n",
    "new_test = new_test.withColumnRenamed(\"item_id\",\"item_id_2\")\n",
    "new_test = new_test.join(item_features, new_test.item_id_2 ==  item_features.item_id,\"inner\") \n",
    "new_test = new_test.drop(\"item_id_2\")\n",
    "#new_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e79499a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we count the number of categories and values for each session\n",
    "b = new_test.groupBy(\"session_id\").agg(countDistinct(\"feature_category_id\").alias(\"number_of_categories_for_session\"), countDistinct(\"feature_value_id\").alias(\"number_of_values_for_session\"))\n",
    "#b.show()\n",
    "#we add these two features to the original dataframe\n",
    "b = b.withColumnRenamed(\"session_id\",\"session_id_2\")\n",
    "df_1 = df_1.join(b, df_1.session_id ==  b.session_id_2,\"inner\") \n",
    "df_1 = df_1.drop(\"session_id_2\")\n",
    "#new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f3952790",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start playing with the time\n",
    "#add epoch as a new feature\n",
    "df_1 = df_1.withColumn( \"epoch_seconds\", \n",
    "      unix_timestamp(col(\"date\"),\"yyyy-MM-dd HH:mm:ss.SSS\").alias(\"date\"))\n",
    "#new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "95d66541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting date string to date\n",
    "df_1 = df_1.withColumn(\"time\", to_timestamp(df_1.date, 'yyyy-MM-dd HH:mm:ss.SSS').alias('dt'))\n",
    "#train_sessions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1e1ea1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofweek\n",
    "df_1 = df_1.withColumn('dayOfWeek', dayofweek(col('time')))\n",
    "df_1 = df_1.withColumn('month', month(col('time')))\n",
    "df_1 = df_1.withColumn('year', year(col('time')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2b98190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_1.withColumn('hour', hour(col('time')))\n",
    "df_1 = df_1.withColumn('minute', minute(col('time')))\n",
    "df_1 = df_1.withColumn('dayofmonth', dayofmonth(col('time')))\n",
    "df_1 = df_1.withColumn('dayofyear', dayofyear(col('time')))\n",
    "df_1 = df_1.withColumn('weekofyear', weekofyear(col('time')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8358e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features = item_features.withColumnRenamed(\"item_id\",\"item_id_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ed2c937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = test_leaderboard_sessions\n",
    "test1 = test1.drop('date','session_id')\n",
    "#test1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "943eed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = test1.groupby(\"item_id\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f8337944",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = test2.withColumnRenamed(\"item_id\",\"item_id_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9652297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we add category ids as features to our dataframe\n",
    "for i in range(1,74):\n",
    "    #print(i)\n",
    "    cond = (test1.item_id == item_features.item_id_2)\n",
    "    test1 = test1.join(item_features, cond, how=\"inner\").withColumn(\"category_id: \" + str(i), when((col(\"feature_category_id\")==i), col(\"feature_value_id\")).otherwise(0))\n",
    "    test1 = test1.drop(\"item_id_2\")\n",
    "    test1 = test1.drop(\"feature_category_id\")\n",
    "    test1 = test1.drop(\"feature_value_id\")\n",
    "    test1 = test1.groupBy(\"item_id\").agg(f.max(\"category_id: \" + str(i)).alias(\"category_id: \" + str(i)))\n",
    "    test2 = test2.join(test1,test2.item_id_2 == test1.item_id, how = \"inner\")\n",
    "    test2 = test2.drop(\"item_id\")\n",
    "    #print('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a8783d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we merge the dataframe with the category ids with the other one that has date and new features\n",
    "final_test = df_1.join(test2, df_1.item_id ==  test2.item_id_2,\"inner\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5a58bb1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('session_id', 'int'),\n",
       " ('item_id', 'int'),\n",
       " ('date', 'string'),\n",
       " ('number_of_sessions', 'bigint'),\n",
       " ('number_of_item_per_session', 'bigint'),\n",
       " ('number_of_items', 'bigint'),\n",
       " ('number_of_session_per_item', 'bigint'),\n",
       " ('number_of_categories', 'bigint'),\n",
       " ('number_of_values', 'bigint'),\n",
       " ('number_of_categories_for_session', 'bigint'),\n",
       " ('number_of_values_for_session', 'bigint'),\n",
       " ('epoch_seconds', 'bigint'),\n",
       " ('time', 'timestamp'),\n",
       " ('dayOfWeek', 'int'),\n",
       " ('month', 'int'),\n",
       " ('year', 'int'),\n",
       " ('hour', 'int'),\n",
       " ('minute', 'int'),\n",
       " ('dayofmonth', 'int'),\n",
       " ('dayofyear', 'int'),\n",
       " ('weekofyear', 'int'),\n",
       " ('item_id_2', 'int'),\n",
       " ('count', 'bigint'),\n",
       " ('category_id: 1', 'int'),\n",
       " ('category_id: 2', 'int'),\n",
       " ('category_id: 3', 'int'),\n",
       " ('category_id: 4', 'int'),\n",
       " ('category_id: 5', 'int'),\n",
       " ('category_id: 6', 'int'),\n",
       " ('category_id: 7', 'int'),\n",
       " ('category_id: 8', 'int'),\n",
       " ('category_id: 9', 'int'),\n",
       " ('category_id: 10', 'int'),\n",
       " ('category_id: 11', 'int'),\n",
       " ('category_id: 12', 'int'),\n",
       " ('category_id: 13', 'int'),\n",
       " ('category_id: 14', 'int'),\n",
       " ('category_id: 15', 'int'),\n",
       " ('category_id: 16', 'int'),\n",
       " ('category_id: 17', 'int'),\n",
       " ('category_id: 18', 'int'),\n",
       " ('category_id: 19', 'int'),\n",
       " ('category_id: 20', 'int'),\n",
       " ('category_id: 21', 'int'),\n",
       " ('category_id: 22', 'int'),\n",
       " ('category_id: 23', 'int'),\n",
       " ('category_id: 24', 'int'),\n",
       " ('category_id: 25', 'int'),\n",
       " ('category_id: 26', 'int'),\n",
       " ('category_id: 27', 'int'),\n",
       " ('category_id: 28', 'int'),\n",
       " ('category_id: 29', 'int'),\n",
       " ('category_id: 30', 'int'),\n",
       " ('category_id: 31', 'int'),\n",
       " ('category_id: 32', 'int'),\n",
       " ('category_id: 33', 'int'),\n",
       " ('category_id: 34', 'int'),\n",
       " ('category_id: 35', 'int'),\n",
       " ('category_id: 36', 'int'),\n",
       " ('category_id: 37', 'int'),\n",
       " ('category_id: 38', 'int'),\n",
       " ('category_id: 39', 'int'),\n",
       " ('category_id: 40', 'int'),\n",
       " ('category_id: 41', 'int'),\n",
       " ('category_id: 42', 'int'),\n",
       " ('category_id: 43', 'int'),\n",
       " ('category_id: 44', 'int'),\n",
       " ('category_id: 45', 'int'),\n",
       " ('category_id: 46', 'int'),\n",
       " ('category_id: 47', 'int'),\n",
       " ('category_id: 48', 'int'),\n",
       " ('category_id: 49', 'int'),\n",
       " ('category_id: 50', 'int'),\n",
       " ('category_id: 51', 'int'),\n",
       " ('category_id: 52', 'int'),\n",
       " ('category_id: 53', 'int'),\n",
       " ('category_id: 54', 'int'),\n",
       " ('category_id: 55', 'int'),\n",
       " ('category_id: 56', 'int'),\n",
       " ('category_id: 57', 'int'),\n",
       " ('category_id: 58', 'int'),\n",
       " ('category_id: 59', 'int'),\n",
       " ('category_id: 60', 'int'),\n",
       " ('category_id: 61', 'int'),\n",
       " ('category_id: 62', 'int'),\n",
       " ('category_id: 63', 'int'),\n",
       " ('category_id: 64', 'int'),\n",
       " ('category_id: 65', 'int'),\n",
       " ('category_id: 66', 'int'),\n",
       " ('category_id: 67', 'int'),\n",
       " ('category_id: 68', 'int'),\n",
       " ('category_id: 69', 'int'),\n",
       " ('category_id: 70', 'int'),\n",
       " ('category_id: 71', 'int'),\n",
       " ('category_id: 72', 'int'),\n",
       " ('category_id: 73', 'int')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1064cea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_test_rdd = final_test.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cc03a7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_test1 = final_test.drop('epoch_seconds','time','date','dayOfWeek','month','year','hour','minute','dayofmonth','dayofyear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7f893e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_test1 = final_test.drop('date','item_id_2','count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9239ec81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some2 = final_test1.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ce6ae993",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some2.to_csv('leaderboard_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ede082",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some.to_csv('final_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3bacc4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some = final_test1.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "71499e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some.to_csv('final_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
